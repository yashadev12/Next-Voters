{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d54dffeb",
   "metadata": {},
   "source": [
    "# **Political Concept Classifer Fine Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc1c504",
   "metadata": {},
   "source": [
    "### The Logic \n",
    "\n",
    "1. We will be using the sloth to finetune the model \n",
    "2. PEFT will allow us to add LoRA adapters which will allow us to finetune the model on a smaller dataset\n",
    "3. TRT will allow us to add the techinical configurations needed\n",
    "4. We will download the GUFF file and save it in the political_concept_classifer folder\n",
    "5. Upload the downloaded model to Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4443035b",
   "metadata": {},
   "source": [
    "### GPU check\n",
    "Ensure that CUDA is available with a performant GPU for fast fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad330a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For GPU check\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467223bc",
   "metadata": {},
   "source": [
    "### Download all required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c677ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -y unsloth peft\n",
    "%pip install unsloth trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43822f0d",
   "metadata": {},
   "source": [
    "### Load pretrained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae3c333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = 'unsloth/Phi-3-mini-4k-instruct-bnb-4bit',\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29adc612",
   "metadata": {},
   "source": [
    "### Preprocess data \n",
    "Tokenize data into vector embeddings and program the expected input (text) and output (category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855456a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "with open(\"training.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "ds = Dataset.from_list(data)\n",
    "\n",
    "def to_text(ex):\n",
    "    resp = ex[\"category\"]\n",
    "    \n",
    "    if not isinstance(resp, str):\n",
    "        resp = json.dumps(resp, ensure_ascii=False)\n",
    "\n",
    "    msgs = [\n",
    "        {\"role\": \"user\", \"content\": ex[\"text\"]},\n",
    "        {\"role\": \"assistant\", \"content\": resp},\n",
    "    ]\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            msgs, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "    }\n",
    "\n",
    "dataset = ds.map(to_text, remove_columns=ds.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7bc553",
   "metadata": {},
   "source": [
    "### Add PEFT LoRA adapter\n",
    "This modifies specific layers instead of the whole model, allowing us to fine tune a LLM with a small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b6d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64,  # rank of matrices (for LoRA)\n",
    "    target_modules=[\n",
    "        'q_proj', 'k_proj', 'v_proj', 'o_proj',\n",
    "        'gate_proj', 'up_proj', 'down_proj',\n",
    "    ],  # which layers to inject LoRA into\n",
    "    lora_alpha = 64 * 2,  # scaling factor, usually 2x rank\n",
    "    lora_dropout = 0,  # no dropout, increase for regularizaiton\n",
    "    bias = 'none',  # bias stays frozen, only learn the low-rank matrices\n",
    "    use_gradient_checkpointing = 'unsloth',  # activate custom checkpointing scheme of Unsloth -> higher compute but less GPU memory when backpropagating\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fdfa13",
   "metadata": {},
   "source": [
    "### Add training configurations\n",
    "Training configurations needed to complete fine tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a124bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "trainer = SFTTrainer(  # supervised fine-tuning trainer\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    dataset_text_field = 'text',\n",
    "    max_seq_length = 2048,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,  # each GPU reads 2 tokenized sequences at once\n",
    "        gradient_accumulation_steps = 4,  # accumulate loss for 4 iterations before optimizer step -> effective batch 2 * 4 = 8\n",
    "        warmup_steps = 10,  # linearly \"climb\" to the learning rate from 0 in the first 10 steps\n",
    "        max_steps = 60,  # max steps before stopping (unless epochs out before that)\n",
    "        logging_steps = 1,  # log every single step\n",
    "        output_dir = \"outputs\",  # where to store checkpoints, logs etc.\n",
    "        optim = \"adamw_8bit\",  # 8-bit AdamW optimizer\n",
    "        num_train_epochs = 3  # number of epochs, unless we reach 60 steps first\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38926b28",
   "metadata": {},
   "source": [
    "### Fine tune the model\n",
    "This one-liner will do all the fine tuning computation needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb383df",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9193c91b",
   "metadata": {},
   "source": [
    "### Test model \n",
    "Check for its accuracy on the `test.json` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc565e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "with open(\"training.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "correct = 0\n",
    "total = data.__len__()\n",
    "\n",
    "for item in data:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": item[\"text\"]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Turn messages to tensor and send to GPU\n",
    "    inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate model response with max 512 tokens and 0.7 temperature, smallest set of tokens with cumulative probability of >= 0.9 are kept for random sampling\n",
    "    outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True, temperature=0.7, do_sample=True, top_p=0.9)\n",
    "\n",
    "    response = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "    if (response == item[\"category\"]):\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy: {correct}/{total} = {correct/total:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a51e0cc",
   "metadata": {},
   "source": [
    "### Save model as GGUF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd58f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained_gguf(\n",
    "    \"gguf_model_scratch_fixed\", \n",
    "    tokenizer, \n",
    "    quantization_method=\"q4_k_m\", \n",
    "    maximum_memory_usage = 0.3\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
